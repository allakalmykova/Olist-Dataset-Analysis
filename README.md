# Анализ Olist

Данная работа является моим финальным проектом для курса по дата аналитике в Moscow Digital School. 

## Введение

База данных Olist - это данные о заказах, сделанных в Olist Store (бразильский вариант озона). Набор данных содержит информацию о 100 тысяч заказов с 2016 по 2018 год, сделанных на нескольких торговых площадках в Бразилии. Данный датасет позволяет просматривать заказ с нескольких сторон: от статуса заказа, цены и оплаты до местоположения клиента, атрибутов продукта и отзывов, написанных клиентами. 

База данных у меня загружена в SQL-server, который я подключила к Excel, Python и Power BI. Практически вся работа была проделана в Python и представлена в Power BI, за исключением одного запроса SQL.

В данном проекте я анализировала оценки продуктов, кол-во покупателей и продавцов по штатам Бразилии, время доставки товаров, а также сделала небольшой отчет по выручке и сделкам компании.

Т.к. продажи в 2016 году являлись нерегулярными, для анализа я работала с данными с 2017 года.

Я старалась сделать свой дэшборд максимально легко читаемым и удобным для пользователя.


## Ключевые факторы

Первая страница моего дэшборда показывает некую общую информацию о датасете: 

<img width="925" alt="Страница 1 - Ключевые факторы" src="https://user-images.githubusercontent.com/114583379/193861342-06a5aeca-201b-42f6-a0f7-c7597bf6cb63.PNG">

Здесь мы видим, что большинство покупок совершаются с помощью кредитной карты. Дальше мы эту информацию не используем, но она все равно дает нам некоторое понимание о процессе оплат. 

Мы видим выручку в долларах за 2017 и 2018 года (перевод с бразильского реала в доллары я осуществляла мерой через подключение к курсу в интернете), среднюю оценку пользователей, и еще некоторые графики. 

### Модель проекта

В проекте я много работала с файлами csv, выгружаемыми после обработки данных в python. Затем я обрабатывала их в Power BI, изменяя типы или удаляя ненужные данные. Я избегала двусторонних связей, так что в проекте нет связей типа "многие ко многим". Финальная модель выглядит следующим образом: 

<img width="733" alt="Модель" src="https://user-images.githubusercontent.com/114583379/193885464-a8efccd8-843b-44ee-a089-50a5fae5db87.PNG">

Мы видим связанные данные, созданный справочник дат, а также справа некоторые таблицы без связей, например курс (источник - интернет), или таблицы для каких-то отдельных графиков.

### SQL запрос

Я сделала один запрос в SQL (см. изображение ниже) для получения кол-ва проданных товаров по категориям, после чего я переключилась на python, где и осталась работать в течение всего проекта.

<img width="1124" alt="SQL запрос" src="https://user-images.githubusercontent.com/114583379/193884528-cb04b4a1-ccc3-4a6e-9597-c1c2fc8fae99.PNG">

### Подключение датасета к python

Сначала я подключила сервер к питону, и загрузила нужные таблицы в датафреймы:

<img width="749" alt="Подключение к питону" src="https://user-images.githubusercontent.com/114583379/193863154-9d3b41e3-8433-469a-b2f7-3a62f0509430.PNG">

## Анализ отзывов покупателей

Вторая страница дэшборда фокусируется на оценках, поставленных товарам покупателями:

<img width="925" alt="Страница 2 - Анализ оценок" src="https://user-images.githubusercontent.com/114583379/193896739-d7f09064-7c1b-44a7-8b61-6c640fb2f4e5.PNG">

В левом верхнем углу мы видим ту же таблицу с категориями, что и на странице с ключевыми факторами. Затем мы видим похожую таблицу, но только по тем товарам, что получили самую худщую оценку. В середине также мы снова видим кольцевой график с оценками, выше него фильтры по кол-ву проданных товаров и по категории. Ниже находится разброс средних оценок по категориям. 

Особый интерес у меня вызвала категория с офисной мебелью:

<img width="926" alt="Страница 2 - Анализ оценок - office furniture" src="https://user-images.githubusercontent.com/114583379/193896746-2631bf19-8fa1-42bd-aa80-34af6586ae07.PNG">

Данная категория получила очень низкую среднюю оценку. В левом нижнем углу мы видим график, показывающий время доставки по этапам. В сравнении со всеми остальными категориями, офисная мебель дольше всех находится в стадии доставки "от подтверждения до склада". Вероятно, из-за этого товары опаздывают, и покупатели оставляют негативные отзывы. 

### Код для вычисления средних оценок

Сначала проверяем наши данные пустые значения, и при необходимости, убираем их:

<img width="742" alt="Проверка order_reviews на дубли" src="https://user-images.githubusercontent.com/114583379/193901639-ddf6569b-b611-46dd-aa69-00e7943ad11f.PNG">

Соединяем нужные таблицы, оставляя нужные нам столбцы, и удаляем дубли (в данном случае, сначала соединяем данные о заказе и о продуктах):

<img width="727" alt="ord_products" src="https://user-images.githubusercontent.com/114583379/193901643-941b7c2a-bf1b-486c-9e37-97ac26f74e64.PNG">

Соединяем с данными об отзывах:

<img width="723" alt="ord_product_reviews" src="https://user-images.githubusercontent.com/114583379/193901649-8578fdc2-f5d5-4503-86b8-bd63b133a2fd.PNG">

Проверяем на пустые значения, и видим, что они присутствуют. Убираем все строки, где есть пустые значения, не считая столбцы с комментариями:

<img width="733" alt="ord_product_reviews убираем дубли" src="https://user-images.githubusercontent.com/114583379/193901662-9b11a05e-6e38-45a1-9e4b-f5b3026f0625.PNG">

Затем группируем по категориям, чтобы получить средние оценки по категориям, и сохраняем в csv файл, который затем будем подгружать в Power BI:

<img width="734" alt="Оценки по категориям и сохранение файла" src="https://user-images.githubusercontent.com/114583379/193901667-527924f2-cf4c-4cf5-831d-ea7450ff71d2.PNG">

## Анализ продавцов и покупателей по штатам

Третья страница дэшборда посвящена распределению кол-ва покупателей и продавцом по штатам Бразилии: 

<img width="926" alt="Страница 3 - Анализ покупателей и продавцов по штатам" src="https://user-images.githubusercontent.com/114583379/193904102-d1e4b169-26d5-4048-86f2-b84d3334ca79.PNG">

Для того, чтобы создать такую карту, нужны были данные о широте и долготе, которые я получила, выявив средние позиции по штатам из таблицы geolocation. Затем соединила эти данные с данными о покупателях: 

<img width="550" alt="Координаты штатов и покупателей" src="https://user-images.githubusercontent.com/114583379/193904722-385bed9b-41cd-445b-bfa0-93ec0bc4dab2.PNG">

Таким же образом я поступила с продавцами. Размеры пузырьков на карте соответствуют относительному кол-ву покупателей в штате.

## Анализ времени доставки

На четвертой странице дэшборда представлен анализ времени доставки товаров по городам:

<img width="920" alt="Страница 4 - Анализ времени доставки товаров" src="https://user-images.githubusercontent.com/114583379/193905899-4d981a64-dbb9-43fc-90b6-8645b5c1ad2c.PNG">

В рамках этого анализа было проделано много работы в python, давайте ее разберем.

1. Создаем координаты по почтовым индексам (берем среднее по каждому индексу):
<img width="583" alt="4 1 Координаты по индексам" src="https://user-images.githubusercontent.com/114583379/193906853-af52663e-8938-4c75-b83e-ce3fc0ea2107.PNG">

2. Соединяем таблицы customers, coordinates в customers2
<img width="720" alt="4 2 customers2" src="https://user-images.githubusercontent.com/114583379/193907281-c3862a67-6be6-4ffe-aa57-e4eaa699282f.PNG">

3. Соединяем таблицы sellers, coordinates в sellers2
<img width="551" alt="4 3 sellers2" src="https://user-images.githubusercontent.com/114583379/193907287-83a14b53-1f23-4f5f-a2da-8e55ee9ccca8.PNG">

4. Соединяем таблицы orders, order items в orders2
<img width="721" alt="4 4 orders2" src="https://user-images.githubusercontent.com/114583379/193907291-2e420f25-24f5-419e-b254-b2c6fa60e81b.PNG">

5. Меняем в orders2 тип дат на дату и добавляем столбцы по датам без времени
<img width="661" alt="4 5 orders2 types" src="https://user-images.githubusercontent.com/114583379/193907840-1e0509a1-2090-4cd1-8a6a-e8f94db0c1d2.PNG">

6. Соединяем таблицы orders2, customers2 в cust_seller чтобы получить таблицу с данными продавцов и покупателей
<img width="729" alt="4 6 cust_seller" src="https://user-images.githubusercontent.com/114583379/193908213-f7edb92d-8c3a-4853-a1df-fdddb05565af.PNG">

7. Соединяем cust_seller с sellers2 чтобы добавить больше информации о продавцах в таблицу deliveries - теперь есть таблица с данными о доставках, покупателях и продавцах
<img width="728" alt="4 7 deliveries" src="https://user-images.githubusercontent.com/114583379/193908218-136886f5-10f7-4733-b852-9fb97bb2a5db.PNG">

8. Обрабатываем deliveries (меняем тип дат, убираем ненужные столбцы, убираем дубли, добавляем столбцы с кол-вом дней доставки, кол-вом дней ожидаемой доставки и кол-вом дней опоздания)
<img width="727" alt="4 8 deliveries worked" src="https://user-images.githubusercontent.com/114583379/193908692-ecb87f36-6d1d-45bb-a65d-e56073ce2a20.PNG">

9. Переводим получившиеся ранее вычисления в дни и создаем таблицу с доставленными заказами delivered  - убираем строки где день доставки null из deliveries.
<img width="734" alt="4 9 delivered" src="https://user-images.githubusercontent.com/114583379/193909099-770d4a52-bbd3-45e9-8f63-cae96420550c.PNG">

10. Создаем таблицу и график с распределением дней доставки всех доставленных заказов (delivered_count)

11. Создаем таблицу и график с распределением дней опоздания опоздавших заказов (late_deliveries_count)

12. Наши данные включают в себя выбросы, что влияет на наши средние значения. Создаем функцию для исключения выбросов filter_outliers

13. Исключаем выбросы для обеих таблиц и сохраняем обе в csv (late_deliveries_no_outliers_count, delivered_no_outliers_count)




