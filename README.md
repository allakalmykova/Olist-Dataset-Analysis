# Анализ Olist

Данная работа является моим финальным проектом для курса по дата аналитике в Moscow Digital School. 

## Введение

База данных Olist - это данные о заказах, сделанных в Olist Store (бразильский онлайн маркетплейс). Набор данных содержит информацию о 100 тысяч заказов с 2016 по 2018 год, сделанных на нескольких торговых площадках в Бразилии. Данный датасет позволяет просматривать заказ с нескольких сторон: от статуса заказа, цены и оплаты до местоположения клиента, атрибутов продукта и отзывов, написанных клиентами. 

База данных у меня загружена в SQL-server, который я подключила к Excel, Python и Power BI. Практически вся работа была проделана в Python и представлена в Power BI, за исключением одного запроса SQL.

В данном проекте я анализировала оценки продуктов, кол-во покупателей и продавцов по штатам Бразилии, время доставки товаров, а также сделала небольшой отчет по выручке и сделкам компании.

Т.к. продажи в 2016 году являлись нерегулярными, для анализа я работала с данными с 2017 года.

Я старалась сделать свой дэшборд максимально легко читаемым и удобным для пользователя.


## Ключевые факторы

Первая страница моего дэшборда показывает некую общую информацию о датасете: 

<img width="925" alt="Страница 1 - Ключевые факторы" src="https://user-images.githubusercontent.com/114583379/193861342-06a5aeca-201b-42f6-a0f7-c7597bf6cb63.PNG">

Здесь мы видим, что большинство покупок совершаются с помощью кредитной карты. Дальше мы эту информацию не используем, но она все равно дает нам некоторое понимание о процессе оплат. 

Мы видим выручку в долларах за 2017 и 2018 года (перевод с бразильского реала в доллары я осуществляла мерой через подключение к курсу в интернете), среднюю оценку пользователей, и еще некоторые графики. 

### Модель проекта

В проекте я много работала с файлами csv, выгружаемыми после обработки данных в python. Затем я обрабатывала их в Power BI, изменяя типы или удаляя ненужные данные. Я избегала двусторонних связей, так что в проекте нет связей типа "многие ко многим". Финальная модель выглядит следующим образом: 

<img width="733" alt="Модель" src="https://user-images.githubusercontent.com/114583379/193885464-a8efccd8-843b-44ee-a089-50a5fae5db87.PNG">

Мы видим связанные данные, созданный справочник дат, а также справа некоторые таблицы без связей, например курс (источник - интернет), или таблицы для каких-то отдельных графиков.

### SQL запрос

Я сделала один запрос в SQL (см. изображение ниже) для получения кол-ва проданных товаров по категориям, после чего я переключилась на python, где и осталась работать в течение всего проекта.

<img width="1124" alt="SQL запрос" src="https://user-images.githubusercontent.com/114583379/193884528-cb04b4a1-ccc3-4a6e-9597-c1c2fc8fae99.PNG">

### Подключение датасета к python

Сначала я подключила сервер к питону, и загрузила нужные таблицы в датафреймы:

<img width="749" alt="Подключение к питону" src="https://user-images.githubusercontent.com/114583379/193863154-9d3b41e3-8433-469a-b2f7-3a62f0509430.PNG">

## Анализ отзывов покупателей

Вторая страница дэшборда фокусируется на оценках, поставленных товарам покупателями:

<img width="925" alt="Страница 2 - Анализ оценок" src="https://user-images.githubusercontent.com/114583379/193896739-d7f09064-7c1b-44a7-8b61-6c640fb2f4e5.PNG">

В левом верхнем углу мы видим ту же таблицу с категориями, что и на странице с ключевыми факторами. Затем мы видим похожую таблицу, но только по тем товарам, что получили самую худщую оценку. В середине также мы снова видим кольцевой график с оценками, выше него фильтры по кол-ву проданных товаров и по категории. Ниже находится разброс средних оценок по категориям. 

Особый интерес у меня вызвала категория с офисной мебелью:

<img width="926" alt="Страница 2 - Анализ оценок - office furniture" src="https://user-images.githubusercontent.com/114583379/193896746-2631bf19-8fa1-42bd-aa80-34af6586ae07.PNG">

Данная категория получила очень низкую среднюю оценку. В левом нижнем углу мы видим график, показывающий время доставки по этапам. В сравнении со всеми остальными категориями, офисная мебель дольше всех находится в стадии доставки "от подтверждения до склада". Вероятно, из-за этого товары опаздывают, и покупатели оставляют негативные отзывы. 

### Код для вычисления средних оценок

Сначала проверяем наши данные пустые значения, и при необходимости, убираем их:

<img width="742" alt="Проверка order_reviews на дубли" src="https://user-images.githubusercontent.com/114583379/193901639-ddf6569b-b611-46dd-aa69-00e7943ad11f.PNG">

Соединяем нужные таблицы, оставляя нужные нам столбцы, и удаляем дубли (в данном случае, сначала соединяем данные о заказе и о продуктах):

<img width="727" alt="ord_products" src="https://user-images.githubusercontent.com/114583379/193901643-941b7c2a-bf1b-486c-9e37-97ac26f74e64.PNG">

Соединяем с данными об отзывах:

<img width="723" alt="ord_product_reviews" src="https://user-images.githubusercontent.com/114583379/193901649-8578fdc2-f5d5-4503-86b8-bd63b133a2fd.PNG">

Проверяем на пустые значения, и видим, что они присутствуют. Убираем все строки, где есть пустые значения, не считая столбцы с комментариями:

<img width="733" alt="ord_product_reviews убираем дубли" src="https://user-images.githubusercontent.com/114583379/193901662-9b11a05e-6e38-45a1-9e4b-f5b3026f0625.PNG">

Затем группируем по категориям, чтобы получить средние оценки по категориям, и сохраняем в csv файл, который затем будем подгружать в Power BI:

<img width="734" alt="Оценки по категориям и сохранение файла" src="https://user-images.githubusercontent.com/114583379/193901667-527924f2-cf4c-4cf5-831d-ea7450ff71d2.PNG">

## Анализ продавцов и покупателей по штатам

Третья страница дэшборда посвящена распределению кол-ва покупателей и продавцом по штатам Бразилии: 

<img width="926" alt="Страница 3 - Анализ покупателей и продавцов по штатам" src="https://user-images.githubusercontent.com/114583379/193904102-d1e4b169-26d5-4048-86f2-b84d3334ca79.PNG">

Для того, чтобы создать такую карту, нужны были данные о широте и долготе, которые я получила, выявив средние позиции по штатам из таблицы geolocation. Затем соединила эти данные с данными о покупателях: 

<img width="550" alt="Координаты штатов и покупателей" src="https://user-images.githubusercontent.com/114583379/193904722-385bed9b-41cd-445b-bfa0-93ec0bc4dab2.PNG">

Таким же образом я поступила с продавцами. Размеры пузырьков на карте соответствуют относительному кол-ву покупателей в штате.

## Анализ времени доставки

На четвертой странице дэшборда представлен анализ времени доставки товаров по городам:

<img width="920" alt="Страница 4 - Анализ времени доставки товаров" src="https://user-images.githubusercontent.com/114583379/193905899-4d981a64-dbb9-43fc-90b6-8645b5c1ad2c.PNG">

В рамках этого анализа было проделано много работы в python, давайте ее разберем.

### Код для графиков, исключение выбросов

1. Создаем координаты по почтовым индексам (берем среднее по каждому индексу):
<img width="583" alt="4 1 Координаты по индексам" src="https://user-images.githubusercontent.com/114583379/193906853-af52663e-8938-4c75-b83e-ce3fc0ea2107.PNG">

2. Соединяем таблицы customers, coordinates в customers2
<img width="720" alt="4 2 customers2" src="https://user-images.githubusercontent.com/114583379/193907281-c3862a67-6be6-4ffe-aa57-e4eaa699282f.PNG">

3. Соединяем таблицы sellers, coordinates в sellers2
<img width="551" alt="4 3 sellers2" src="https://user-images.githubusercontent.com/114583379/193907287-83a14b53-1f23-4f5f-a2da-8e55ee9ccca8.PNG">

4. Соединяем таблицы orders, order items в orders2
<img width="721" alt="4 4 orders2" src="https://user-images.githubusercontent.com/114583379/193907291-2e420f25-24f5-419e-b254-b2c6fa60e81b.PNG">

5. Меняем в orders2 тип дат на дату и добавляем столбцы по датам без времени
<img width="661" alt="4 5 orders2 types" src="https://user-images.githubusercontent.com/114583379/193907840-1e0509a1-2090-4cd1-8a6a-e8f94db0c1d2.PNG">

6. Соединяем таблицы orders2, customers2 в cust_seller чтобы получить таблицу с данными продавцов и покупателей
<img width="729" alt="4 6 cust_seller" src="https://user-images.githubusercontent.com/114583379/193908213-f7edb92d-8c3a-4853-a1df-fdddb05565af.PNG">

7. Соединяем cust_seller с sellers2 чтобы добавить больше информации о продавцах в таблицу deliveries - теперь есть таблица с данными о доставках, покупателях и продавцах
<img width="728" alt="4 7 deliveries" src="https://user-images.githubusercontent.com/114583379/193908218-136886f5-10f7-4733-b852-9fb97bb2a5db.PNG">

8. Обрабатываем deliveries (меняем тип дат, убираем ненужные столбцы, убираем дубли, добавляем столбцы с кол-вом дней доставки, кол-вом дней ожидаемой доставки и кол-вом дней опоздания)
<img width="727" alt="4 8 deliveries worked" src="https://user-images.githubusercontent.com/114583379/193908692-ecb87f36-6d1d-45bb-a65d-e56073ce2a20.PNG">

9. Переводим получившиеся ранее вычисления в дни и создаем таблицу с доставленными заказами delivered  - убираем строки где день доставки null из deliveries.
<img width="734" alt="4 9 delivered" src="https://user-images.githubusercontent.com/114583379/193909099-770d4a52-bbd3-45e9-8f63-cae96420550c.PNG">

10. Создаем таблицу и график с распределением дней доставки всех доставленных заказов (delivered_count)
<img width="627" alt="4 10 delivered_count" src="https://user-images.githubusercontent.com/114583379/193911973-64eb0d28-8e36-47cc-ba8f-9caf17b3e375.PNG">

11. Создаем таблицу и график с распределением дней опоздания опоздавших заказов (late_deliveries_count)
<img width="626" alt="4 11 late_deliveries_count" src="https://user-images.githubusercontent.com/114583379/193911975-d27eea94-a005-4960-92a8-92e305015571.PNG">

12. Наши данные включают в себя выбросы, что влияет на наши средние значения. Создаем функцию для исключения выбросов filter_outliers
<img width="645" alt="4 12 filter_outliers" src="https://user-images.githubusercontent.com/114583379/193911982-84fdbf9f-9838-478f-b24c-90072bb7d6a3.PNG">

13. Исключаем выбросы для обеих таблиц и сохраняем обе в csv (late_deliveries_no_outliers_count, delivered_no_outliers_count)
<img width="636" alt="4 12 filter_outliers2" src="https://user-images.githubusercontent.com/114583379/193911994-2446c669-7d0a-427a-bbeb-47c2972cfe18.PNG">

### Код для сортировки по городам

1. Убираем из deliveries найденную строку с неверно заданным городом продавца
2. Группируем по городам кол-во заказов (две таблицы - одну по всем заказам, одну по опоздавшим)
<img width="641" alt="4 13 города" src="https://user-images.githubusercontent.com/114583379/193913629-08b91a64-ba7a-4901-bcd7-41eeea23cfa1.PNG">

3. Получаем средние координаты по городам
<img width="415" alt="4 14 coordinates" src="https://user-images.githubusercontent.com/114583379/193913636-f29abae7-6377-4002-b1a8-cd29ac0c33e5.PNG">

4. Соединяем обе таблицы, добавляем координаты, вычисляем процент опоздавших. Для анализа я взяла топ 50 городов по кол-ву заказов. Сортируем по проценту опоздавших  заказов и сохраням в csv - deliveries_seller_city_top50.csv
<img width="647" alt="4 15 cities final" src="https://user-images.githubusercontent.com/114583379/193913638-a46fcdfe-f33c-4a5f-bd19-fc39a72fa71b.PNG">

## Выручка

В свое оставшееся время я сделала небольшие отчеты по выручке и сделкам. Отчет по выручке:

<img width="927" alt="Страница 5 - Анализ выручки" src="https://user-images.githubusercontent.com/114583379/193914439-cd9ef6db-a7f8-4c58-94b5-1407be3ef78f.PNG">

По данному отчету виден динамичный рост.

Здесь наглядно видно, что в ноябре 2017 года был скачок. Если нажать на ноябрь на странице "Ключевые Факторы", мы также увидим, что большинство продаж приходится на пятницу. Очевидно, что были хорошо разрекламированы скидки на черную пятницу.

В таблице "средний процент расходов по категориям" мне бросилось в глаза, что первое место занимаем категория housewares, хотя по выручке она на 7 месте. При более глубоком анализе, можно было бы изучить природу данных расходов.

## Сделки 

Отчет по сделкам:
<img width="926" alt="Страница 6 - Анализ сделок" src="https://user-images.githubusercontent.com/114583379/193914446-f108aec1-ca1b-4275-9519-90550d07b419.PNG">

Топ сделка:
<img width="924" alt="Страница 6 - Анализ сделок - топ сделка" src="https://user-images.githubusercontent.com/114583379/193914452-838434c0-0c1d-486a-94fb-098370142417.PNG">

Самая большая сделка была заключена в феврале 2018 года, что отразилось на продажах. Руководство компании, очевидно, заметили потенциал в категории watches_gifts и решили ее продвинуть, что было очень успешным решением.

## Заключение

В данном проекте большой фокус был на работе в питоне для анализа времени доставки товаров и отзывов покупателей. 

В моей следующей работе в Power BI (смотреть репозиторий SalesAnalytics) я начала сокращать кол-во таблиц, с которым работаю, и использовать больше мер - работая с большими количествами данных очень важно использовать как можно меньше таблиц и столбцов.

Спасибо за уделенное время!
